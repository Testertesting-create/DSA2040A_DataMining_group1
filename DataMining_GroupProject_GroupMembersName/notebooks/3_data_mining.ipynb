{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Analysis\n",
    "\n",
    "**Contributors:** [Add team member names]\n",
    "\n",
    "This notebook implements advanced data mining techniques on the OSMI Mental Health Tech Survey dataset to uncover patterns, predict outcomes, and identify relationships in mental health data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, accuracy_score\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecomposition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlxtend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfrequent_patterns\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apriori, association_rules\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m     15\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('../data/transformed/OSMI_Mental_Health_Final.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing for Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for mining\n",
    "df_mining = df.copy()\n",
    "\n",
    "# Select relevant features for mining\n",
    "mining_features = ['Age', 'Gender', 'treatment', 'family_history', 'age_group']\n",
    "df_mining = df_mining[mining_features].copy()\n",
    "\n",
    "# Handle missing values\n",
    "print(\"Missing values before preprocessing:\")\n",
    "print(df_mining.isnull().sum())\n",
    "\n",
    "# Fill missing values with mode for categorical variables\n",
    "categorical_cols = df_mining.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    df_mining[col] = df_mining[col].fillna(df_mining[col].mode()[0])\n",
    "\n",
    "# Fill missing values with median for numerical variables\n",
    "numerical_cols = df_mining.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    df_mining[col] = df_mining[col].fillna(df_mining[col].median())\n",
    "\n",
    "print(\"\\nMissing values after preprocessing:\")\n",
    "print(df_mining.isnull().sum())\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_mining[col + '_encoded'] = le.fit_transform(df_mining[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"{col} encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "# Create feature matrix for clustering\n",
    "feature_cols = ['Age'] + [col + '_encoded' for col in categorical_cols if col != 'Age']\n",
    "X_clustering = df_mining[feature_cols].copy()\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X_clustering.shape}\")\n",
    "print(f\"Features: {list(X_clustering.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features for clustering\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clustering)\n",
    "\n",
    "# Find optimal number of clusters using elbow method\n",
    "inertias = []\n",
    "K_range = range(1, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(K_range, inertias, 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True)\n",
    "\n",
    "# Calculate silhouette scores\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_scores = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(2, 11), silhouette_scores, 'ro-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Silhouette scores: {silhouette_scores}\")\n",
    "optimal_k = range(2, 11)[np.argmax(silhouette_scores)]\n",
    "print(f\"Optimal number of clusters: {optimal_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with optimal k\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to original dataframe\n",
    "df_mining['Cluster'] = cluster_labels\n",
    "\n",
    "# Analyze clusters\n",
    "print(\"=== CLUSTER ANALYSIS ===\")\n",
    "print(f\"Number of clusters: {optimal_k}\")\n",
    "print(f\"Cluster sizes:\")\n",
    "for i in range(optimal_k):\n",
    "    cluster_size = (df_mining['Cluster'] == i).sum()\n",
    "    percentage = (cluster_size / len(df_mining)) * 100\n",
    "    print(f\"  Cluster {i}: {cluster_size} ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize clusters using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis')\n",
    "plt.title('K-Means Clustering Results (PCA)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "# Cluster characteristics\n",
    "plt.subplot(1, 2, 2)\n",
    "cluster_means = df_mining.groupby('Cluster')[['Age', 'Gender_encoded', 'treatment_encoded', 'family_history_encoded']].mean()\n",
    "sns.heatmap(cluster_means.T, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Cluster Characteristics (Normalized)')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed cluster analysis\n",
    "print(\"\\n=== DETAILED CLUSTER CHARACTERISTICS ===\")\n",
    "for i in range(optimal_k):\n",
    "    cluster_data = df_mining[df_mining['Cluster'] == i]\n",
    "    print(f\"\\nCluster {i}:\")\n",
    "    print(f\"  Size: {len(cluster_data)} ({len(cluster_data)/len(df_mining)*100:.1f}%)\")\n",
    "    print(f\"  Average Age: {cluster_data['Age'].mean():.1f}\")\n",
    "    print(f\"  Treatment Rate: {(cluster_data['treatment'] == 'yes').mean()*100:.1f}%\")\n",
    "    print(f\"  Family History Rate: {(cluster_data['family_history'] == 'yes').mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try DBSCAN clustering\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Find optimal epsilon\n",
    "neighbors = NearestNeighbors(n_neighbors=5)\n",
    "neighbors_fit = neighbors.fit(X_scaled)\n",
    "distances, indices = neighbors_fit.kneighbors(X_scaled)\n",
    "distances = np.sort(distances[:, 4])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(distances)\n",
    "plt.xlabel('Points')\n",
    "plt.ylabel('Distance')\n",
    "plt.title('K-Distance Graph for DBSCAN')\n",
    "plt.grid(True)\n",
    "\n",
    "# Try different eps values\n",
    "eps_values = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "n_clusters_dbscan = []\n",
    "n_noise = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=5)\n",
    "    labels = dbscan.fit_predict(X_scaled)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_points = list(labels).count(-1)\n",
    "    n_clusters_dbscan.append(n_clusters)\n",
    "    n_noise.append(n_noise_points)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(eps_values, n_clusters_dbscan, 'bo-')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Number of Clusters')\n",
    "plt.title('DBSCAN Clusters vs Epsilon')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"DBSCAN Results:\")\n",
    "for eps, n_clusters, noise in zip(eps_values, n_clusters_dbscan, n_noise):\n",
    "    print(f\"Epsilon {eps}: {n_clusters} clusters, {noise} noise points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Predict Treatment Seeking Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for classification\n",
    "X_class = df_mining[['Age', 'Gender_encoded', 'family_history_encoded']].copy()\n",
    "y_class = df_mining['treatment_encoded']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_class, y_class, test_size=0.3, random_state=42, stratify=y_class)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Class distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "# Scale features\n",
    "scaler_class = StandardScaler()\n",
    "X_train_scaled = scaler_class.fit_transform(X_train)\n",
    "X_test_scaled = scaler_class.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple classification models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Cross-validation: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[name]['accuracy'] for name in model_names]\n",
    "plt.bar(model_names, accuracies)\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "cv_means = [results[name]['cv_mean'] for name in model_names]\n",
    "cv_stds = [results[name]['cv_std'] for name in model_names]\n",
    "plt.bar(model_names, cv_means, yerr=cv_stds, capsize=5)\n",
    "plt.title('Cross-Validation Scores')\n",
    "plt.ylabel('CV Score')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance for Random Forest\n",
    "best_model = models['Random Forest']\n",
    "feature_importance = best_model.feature_importances_\n",
    "feature_names = X_class.columns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(feature_names, feature_importance)\n",
    "plt.title('Feature Importance (Random Forest)')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "for feature, importance in zip(feature_names, feature_importance):\n",
    "    print(f\"{feature}: {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Association Rule Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for association rules\n",
    "# Create binary features for association rule mining\n",
    "df_assoc = df_mining[['Gender', 'treatment', 'family_history', 'age_group']].copy()\n",
    "\n",
    "# Create dummy variables\n",
    "df_dummies = pd.get_dummies(df_assoc, prefix=['Gender', 'Treatment', 'FamilyHistory', 'AgeGroup'])\n",
    "\n",
    "print(f\"Association rules dataset shape: {df_dummies.shape}\")\n",
    "print(f\"Features: {list(df_dummies.columns)}\")\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find frequent itemsets\n",
    "frequent_itemsets = apriori(df_dummies, min_support=0.1, use_colnames=True)\n",
    "\n",
    "print(f\"Number of frequent itemsets: {len(frequent_itemsets)}\")\n",
    "print(\"\\nTop 10 frequent itemsets:\")\n",
    "print(frequent_itemsets.sort_values('support', ascending=False).head(10))\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "\n",
    "print(f\"\\nNumber of association rules: {len(rules)}\")\n",
    "print(\"\\nTop 10 rules by confidence:\")\n",
    "print(rules.sort_values('confidence', ascending=False).head(10)[['antecedents', 'consequents', 'support', 'confidence', 'lift']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize association rules\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Support vs Confidence\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(rules['support'], rules['confidence'], alpha=0.5)\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Support vs Confidence')\n",
    "\n",
    "# Support vs Lift\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(rules['support'], rules['lift'], alpha=0.5)\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Lift')\n",
    "plt.title('Support vs Lift')\n",
    "\n",
    "# Confidence vs Lift\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(rules['confidence'], rules['lift'], alpha=0.5)\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Lift')\n",
    "plt.title('Confidence vs Lift')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Filter high-quality rules\n",
    "high_quality_rules = rules[(rules['confidence'] > 0.7) & (rules['lift'] > 1.2)]\n",
    "print(f\"\\nHigh-quality rules (confidence > 0.7, lift > 1.2): {len(high_quality_rules)}\")\n",
    "print(\"\\nTop high-quality rules:\")\n",
    "for idx, rule in high_quality_rules.head(5).iterrows():\n",
    "    print(f\"Rule: {list(rule['antecedents'])} -> {list(rule['consequents'])}\")\n",
    "    print(f\"Support: {rule['support']:.3f}, Confidence: {rule['confidence']:.3f}, Lift: {rule['lift']:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA MINING INSIGHTS SUMMARY ===\")\n",
    "\n",
    "print(\"\\n1. CLUSTERING INSIGHTS:\")\n",
    "print(f\"   - Optimal number of clusters: {optimal_k}\")\n",
    "print(f\"   - Clusters represent different mental health profiles\")\n",
    "print(f\"   - Each cluster has distinct demographic and treatment patterns\")\n",
    "\n",
    "print(\"\\n2. CLASSIFICATION INSIGHTS:\")\n",
    "best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
    "best_accuracy = results[best_model_name]['accuracy']\n",
    "print(f\"   - Best model: {best_model_name} (Accuracy: {best_accuracy:.3f})\")\n",
    "print(f\"   - Key predictors of treatment seeking: Age, Gender, Family History\")\n",
    "print(f\"   - Model can help identify at-risk individuals\")\n",
    "\n",
    "print(\"\\n3. ASSOCIATION RULE INSIGHTS:\")\n",
    "print(f\"   - Number of frequent patterns: {len(frequent_itemsets)}\")\n",
    "print(f\"   - Number of association rules: {len(rules)}\")\n",
    "print(f\"   - High-quality rules: {len(high_quality_rules)}\")\n",
    "print(f\"   - Rules reveal relationships between mental health factors\")\n",
    "\n",
    "print(\"\\n4. BUSINESS IMPLICATIONS:\")\n",
    "print(\"   - Organizations can target interventions based on cluster profiles\")\n",
    "print(\"   - Predictive models can identify employees needing mental health support\")\n",
    "print(\"   - Association rules help understand risk factor combinations\")\n",
    "print(\"   - Data-driven approach to mental health workplace policies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This data mining analysis has successfully applied three key techniques to the mental health dataset:\n",
    "\n",
    "1. **Clustering Analysis**: Identified distinct segments of tech workers with different mental health characteristics\n",
    "2. **Classification**: Built predictive models to identify factors influencing treatment-seeking behavior\n",
    "3. **Association Rules**: Discovered relationships between different mental health factors\n",
    "\n",
    "These insights provide actionable intelligence for organizations looking to improve mental health support in the tech industry.\n",
    "\n",
    "---\n",
    "**Next Steps**: Proceed to `4_insights_dashboard.ipynb` to create interactive visualizations and dashboards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
